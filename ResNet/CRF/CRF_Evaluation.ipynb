{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvips\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "import time, os, copy, datetime, glob\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': array([0.77906426, 0.74919518, 0.77529276]), 'std': array([0.13986633, 0.15931302, 0.17665639])}\n"
     ]
    }
   ],
   "source": [
    "format_to_dtype = {\n",
    "    'uchar': np.uint8,\n",
    "    'char': np.int8,\n",
    "    'ushort': np.uint16,\n",
    "    'short': np.int16,\n",
    "    'uint': np.uint32,\n",
    "    'int': np.int32,\n",
    "    'float': np.float32,\n",
    "    'double': np.float64,\n",
    "    'complex': np.complex64,\n",
    "    'dpcomplex': np.complex128,\n",
    "}\n",
    "\n",
    "# vips image to numpy array\n",
    "def vips2numpy(vi):\n",
    "    return np.ndarray(buffer=vi.write_to_memory(),\n",
    "                      dtype=format_to_dtype[vi.format],\n",
    "                      shape=[vi.height, vi.width, vi.bands])\n",
    "NORM_PATH = '/BrainSeg/normalization.npy'\n",
    "norm = np.load(NORM_PATH,allow_pickle=True).item()\n",
    "print(norm)\n",
    "\n",
    "trans = transforms.Compose([\n",
    "#                               transforms.RandomHorizontalFlip(),\n",
    "#                               transforms.RandomVerticalFlip(),\n",
    "#                               transforms.RandomRotation(180),\n",
    "#                               transforms.ColorJitter(brightness=0.1, contrast=0.2,saturation=0.2, hue=0.02),\n",
    "#                               transforms.RandomAffine(0, translate=(0.05,0.05), scale=(0.9,1.1), shear=10),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(norm['mean'], norm['std'])\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "### comments starting with \"###\" are my (Toluwa's) notes\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_nodes, iteration=10):\n",
    "        \"\"\"Initialize the CRF module\n",
    "        Args:\n",
    "            num_nodes: int, number of nodes/patches within the fully CRF\n",
    "            iteration: int, number of mean field iterations, e.g. 10\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.iteration = iteration\n",
    "        self.W = nn.Parameter(torch.zeros(1, num_nodes, num_nodes))\n",
    "\n",
    "    def forward(self, feats, logits):\n",
    "        \"\"\"Performing the CRF. Algorithm details is explained below:\n",
    "        Within the paper, I formulate the CRF distribution using negative\n",
    "        energy and cost, e.g. cosine distance, to derive pairwise potentials\n",
    "        following the convention in energy based models. But for implementation\n",
    "        simplicity, I use reward, e.g. cosine similarity to derive pairwise\n",
    "        potentials. So now, pairwise potentials would encourage high reward for\n",
    "        assigning (y_i, y_j) with the same label if (x_i, x_j) are similar, as\n",
    "        measured by cosine similarity, pairwise_sim. For\n",
    "        pairwise_potential_E = torch.sum(\n",
    "            probs * pairwise_potential - (1 - probs) * pairwise_potential,\n",
    "            dim=2, keepdim=True\n",
    "        )\n",
    "        This is taking the expectation of pairwise potentials using the current\n",
    "        marginal distribution of each patch being tumor, i.e. probs. There are\n",
    "        four cases to consider when taking the expectation between (i, j):\n",
    "        1. i=T,j=T; 2. i=N,j=T; 3. i=T,j=N; 4. i=N,j=N\n",
    "        probs is the marginal distribution of each i being tumor, therefore\n",
    "        logits > 0 means tumor and logits < 0 means normal. Given this, the\n",
    "        full expectation equation should be:\n",
    "        [probs * +pairwise_potential] + [(1 - probs) * +pairwise_potential] +\n",
    "                    case 1                            case 2\n",
    "        [probs * -pairwise_potential] + [(1 - probs) * -pairwise_potential]\n",
    "                    case 3                            case 4\n",
    "        positive sign rewards logits to be more tumor and negative sign rewards\n",
    "        logits to be more normal. But because of label compatibility, i.e. the\n",
    "        indicator function within equation 3 in the paper, case 2 and case 3\n",
    "        are dropped, which ends up being:\n",
    "        probs * pairwise_potential - (1 - probs) * pairwise_potential\n",
    "        In high level speaking, if (i, j) embedding are different, then\n",
    "        pairwise_potential, as computed as cosine similarity, would approach 0,\n",
    "        which then as no affect anyway. if (i, j) embedding are similar, then\n",
    "        pairwise_potential would be a positive reward. In this case,\n",
    "        if probs -> 1, then pairwise_potential promotes tumor probability;\n",
    "        if probs -> 0, then -pairwise_potential promotes normal probability.\n",
    "        Args:\n",
    "            feats: 3D tensor with the shape of\n",
    "            [batch_size, num_nodes, embedding_size], where num_nodes is the\n",
    "            number of patches within a grid, e.g. 9 for a 3x3 grid;\n",
    "            embedding_size is the size of extracted feature representation for\n",
    "            each patch from ResNet, e.g. 512\n",
    "            logits: 3D tensor with shape of [batch_size, num_nodes, 1], the\n",
    "            logit of each patch within the grid being tumor before CRF\n",
    "        Returns:\n",
    "            logits: 3D tensor with shape of [batch_size, num_nodes, 1], the\n",
    "            logit of each patch within the grid being tumor after CRF\n",
    "        \"\"\"\n",
    "        ###We can formulate the above as 0 for bg\n",
    "        ###0.5 for white matter, 1 for grey matter\n",
    "        ###p=2 means nuclear norm\n",
    "        #print(\"input logits are with shape\", logits, logits.shape)\n",
    "        feats_norm = torch.norm(feats, p=2, dim=2, keepdim=True)\n",
    "        pairwise_norm = torch.bmm(feats_norm,\n",
    "                                  torch.transpose(feats_norm, 1, 2))\n",
    "        pairwise_dot = torch.bmm(feats, torch.transpose(feats, 1, 2))\n",
    "        # cosine similarity between feats\n",
    "        pairwise_sim = pairwise_dot / pairwise_norm\n",
    "        # symmetric constraint for CRF weights\n",
    "        W_sym = (self.W + torch.transpose(self.W, 1, 2)) / 2\n",
    "        pairwise_potential = pairwise_sim * W_sym\n",
    "        unary_potential = logits.clone()\n",
    "\n",
    "        for i in range(self.iteration):\n",
    "            # current Q after normalizing the logits\n",
    "            ###probs = torch.transpose(logits.sigmoid(), 1, 2)\n",
    "            #print(\"logits before\", logits.shape, pairwise_potential.shape)\n",
    "            probs = torch.transpose(logits.softmax(2, torch.float32), 1, 2)\n",
    "            #print(\"logits now\", probs, probs.shape, pairwise_potential.shape)\n",
    "            \n",
    "            # taking expectation of pairwise_potential using current Q\n",
    "            ###Toluwa - this may need updating because original was formulated assuming there were only 2 classes\n",
    "            \n",
    "            ##Probability that it belongs to class 0\n",
    "            pairwise_potential_E0 = torch.sum(\n",
    "                probs[:,0:1,:] * pairwise_potential - (1 - probs[:,0:1,:]) * pairwise_potential,\n",
    "                dim=2, keepdim=True)\n",
    "            \n",
    "            ##Probability that it belongs to class 1\n",
    "            pairwise_potential_E1 = torch.sum(\n",
    "            probs[:,1:2,:] * pairwise_potential - (1 - probs[:,0:1,:]) * pairwise_potential,\n",
    "            dim=2, keepdim=True)\n",
    "            \n",
    "            ##Probability that it belongs to class 2          \n",
    "            pairwise_potential_E2 = torch.sum(\n",
    "            probs[:,2:,:] * pairwise_potential - (1 - probs[:,0:1,:]) * pairwise_potential,\n",
    "            dim=2, keepdim=True)\n",
    "            \n",
    "            pairwise_potential_E = torch.cat((pairwise_potential_E0, pairwise_potential_E1), 2)\n",
    "            pairwise_potential_E = torch.cat((pairwise_potential_E, pairwise_potential_E2), 2)\n",
    "            \n",
    "            #print(\"unary potential has shape\", unary_potential.shape, pairwise_potential_E.shape)\n",
    "            logits = unary_potential + pairwise_potential_E\n",
    "\n",
    "        #print(\"Logits shape is\", logits.shape, logits)\n",
    "        return logits\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'CRF(num_nodes={}, iteration={})'.format(\n",
    "            self.num_nodes, self.iteration\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=3, num_nodes=1,\n",
    "                 use_crf=True):\n",
    "        \"\"\"Constructs a ResNet model.\n",
    "        Args:\n",
    "            num_classes: int, since we are doing binary classification\n",
    "                (tumor vs normal), num_classes is set to 1 and sigmoid instead\n",
    "                of softmax is used later\n",
    "            num_nodes: int, number of nodes/patches within the fully CRF\n",
    "            use_crf: bool, use the CRF component or not\n",
    "        \"\"\"\n",
    "        ###Jokes, we're NOT doing binary classification so modify this for multi-classification\n",
    "        ###Mainly we're doing softmax and setting num_classes to 3\n",
    "\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        #print(\"Model debug block\", block, block.expansion)\n",
    "        self.crf = CRF(num_nodes) if use_crf else None\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 5D tensor with shape of\n",
    "            [batch_size, grid_size, 3, crop_size, crop_size],\n",
    "            where grid_size is the number of patches within a grid (e.g. 9 for\n",
    "            a 3x3 grid); crop_size is 224 by default for ResNet input;\n",
    "        Returns:\n",
    "            logits, 2D tensor with shape of [batch_size, grid_size], the logit\n",
    "            of each patch within the grid being tumor\n",
    "        \"\"\"\n",
    "        #print(\"X shape is\", x.shape)\n",
    "        batch_size, grid_size, _, crop_size = x.shape[0:4]\n",
    "        # flatten grid_size dimension and combine it into batch dimension\n",
    "        x = x.view(-1, 3, crop_size, crop_size)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # feats means features, i.e. patch embeddings from ResNet\n",
    "        feats = x.view(x.size(0), -1)\n",
    "        #print(\"feats shape\", feats.shape, feats[0].shape)\n",
    "        #print(\"feats are\", feats)\n",
    "\n",
    "        logits = self.fc(feats)\n",
    "\n",
    "        # restore grid_size dimension for CRF\n",
    "        feats = feats.view((batch_size, grid_size, -1))\n",
    "        logits = logits.view((batch_size, grid_size, -1))\n",
    "\n",
    "        if self.crf:\n",
    "            logits = self.crf(feats, logits)\n",
    "            \n",
    "        #print(\"Final logits shape before squeezeis \", logits.shape, logits)\n",
    "\n",
    "        logits = torch.squeeze(logits)\n",
    "        ##Toluwa adding this to return which class we're picking\n",
    "        ##print(\"Final logits shape is \", logits.shape, logits)\n",
    "        #_, logits = torch.max(logits, 2)\n",
    "        #print(\"Final logits shape 2 is\", logits.shape, logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {'resnet18': resnet18,\n",
    "          'resnet34': resnet34,\n",
    "          'resnet50': resnet50,\n",
    "          'resnet101': resnet101,\n",
    "          'resnet152': resnet152}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "PATCH_SIZE = 3 \n",
    "\n",
    "IMG_HEIGHT = PATCH_SIZE*256\n",
    "IMG_WIDTH  = PATCH_SIZE*256\n",
    "cfg = {\n",
    " \"model\": \"resnet18\",\n",
    " \"use_crf\": True,\n",
    " \"batch_size\": BATCH_SIZE,\n",
    " \"image_size\": IMG_HEIGHT,\n",
    " \"patch_size\": 256,\n",
    " \"crop_size\": 768,\n",
    " \"lr\": 0.0001,\n",
    " \"momentum\": 0.9,\n",
    " \"epoch\": 20,\n",
    " \"log_every\": 100\n",
    "}\n",
    "\n",
    "\n",
    "if cfg['image_size'] % cfg['patch_size'] != 0:\n",
    "    raise Exception('Image size / patch size != 0 : {} / {}'.format(cfg['image_size'], cfg['patch_size']))\n",
    "\n",
    "patch_per_side = cfg['image_size'] // cfg['patch_size']\n",
    "grid_size = patch_per_side * patch_per_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/BrainSeg/Classify_Results/CRF_0913/GMBData/\n",
      "/BrainSeg/Classify_Results/CRF_0913/GMBMasks/\n",
      "Run for the following 1 images out of 30 images:\n",
      " ['NA4945-02_AB17-24.png']\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/BrainSeg/'\n",
    "IMG_DIR  = BASE_DIR + 'norm_png/'\n",
    "SAVE_DATA_DIR = BASE_DIR + 'Classify_Results/CRF_0913/GMBData/'\n",
    "if not os.path.exists(SAVE_DATA_DIR):\n",
    "    os.makedirs(SAVE_DATA_DIR)\n",
    "print(SAVE_DATA_DIR)\n",
    "SAVE_IMG_DIR = BASE_DIR + 'Classify_Results/CRF_0913/GMBMasks/'\n",
    "if not os.path.exists(SAVE_IMG_DIR):\n",
    "    os.makedirs(SAVE_IMG_DIR)\n",
    "print(SAVE_IMG_DIR)\n",
    "\n",
    "filenames = glob.glob(IMG_DIR + '*.png')\n",
    "filenames = [filename.split('/')[-1] for filename in filenames]\n",
    "filenames = sorted(filenames)\n",
    "total_image_num = len(filenames)\n",
    "\n",
    "idx_image_to_eval = slice(21,22)      # Modify this line to select images to eval\n",
    "filenames = filenames[idx_image_to_eval]\n",
    "print('Run for the following %d images out of %d images:\\n' % (len(filenames), total_image_num), filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(image):\n",
    "    image = trans(image)\n",
    "    image = np.array(image, dtype=np.float32).transpose((0, 1, 2))\n",
    "    _patch_size = 256\n",
    "    _crop_size = 224\n",
    "    #flatten it \n",
    "    img_flat = np.zeros(\n",
    "    (9, 3, _crop_size, _crop_size),\n",
    "    dtype=np.float32)\n",
    "\n",
    "    ### This part is from the NCRF code\n",
    "    idx = 0\n",
    "    for x_idx in range(3):\n",
    "        for y_idx in range(3):\n",
    "            # center crop each patch\n",
    "            x_start = int(\n",
    "                (x_idx + 0.5) * _patch_size - _crop_size / 2)\n",
    "            x_end = x_start + _crop_size\n",
    "            y_start = int(\n",
    "                (y_idx + 0.5) * _patch_size - _crop_size / 2)\n",
    "            y_end = y_start + _crop_size\n",
    "            img_flat[idx] = image[:, x_start:x_end, y_start:y_end]\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    return img_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/BrainSeg/Codes/CRF/LF0811_Checkpoints/PatchedCRF_3x3_23.pkl')\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 11178132.0\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "# print(\"Number of parameter: %.6fM\" % (total/1e6))\n",
    "print(\"Number of parameter: %.1f\" % (total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSI = pyvips.Image.new_from_file(IMG_DIR + filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('0')\n",
    "img = WSI.extract_area(0,0,768,768)\n",
    "print('1')\n",
    "# img.write_to_file('/BrainSeg/Codes/CRF/Checkpoints/img.png')\n",
    "# img = Image.open('/BrainSeg/Codes/CRF/Checkpoints/img.png')\n",
    "img = vips2numpy(img)\n",
    "img = Image.fromarray(img, 'RGB')\n",
    "\n",
    "img = act(img)\n",
    "print('4')\n",
    "img = torch.from_numpy(img)\n",
    "print('5')            \n",
    "#             print(tile_img)\n",
    "tile_img = img.unsqueeze(0)\n",
    "\n",
    "tile_img = Variable(tile_img)\n",
    "print('6')\n",
    "tile_img = tile_img.cuda()\n",
    "print('7')\n",
    "predict = model(tile_img)\n",
    "print('8')\n",
    "_, p = torch.max(predict.data, 1)\n",
    "print(p[4].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.0174, -1.8069, -0.2735], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# pred = predict[4,:]\n",
    "print(pred)\n",
    "_, p = torch.max(predict.data, 1)\n",
    "print(p[4].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image NA4945-02_AB17-24.png - (49800, 43128):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image NA4945-02_AB17-24.png - (49800, 43128): 100%|██████████| 1/1 [56:27<00:00, 3387.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Start evaluating\n",
    "t = tqdm(total=len(filenames))\n",
    "# print(filenames)\n",
    "for filename in filenames:\n",
    "  # print(filename)\n",
    "  # in_img = np.array(Image.open(IMG_DIR + filename)) # Out of RAM\n",
    "    in_img = pyvips.Image.new_from_file(IMG_DIR + filename)\n",
    "  \n",
    "    t.set_description_str(\"Image \" + filename + ' - (%d, %d)' % (in_img.width, in_img.height))\n",
    "    t.refresh()\n",
    "    t.write(\"\", end=' ')\n",
    "\n",
    "    num_cols = int((in_img.width-768)/128)\n",
    "    num_rows = int((in_img.height-768)/128)\n",
    "    nums = np.zeros((num_rows, num_cols), dtype='uint8')\n",
    "#     PRED = np.zeros((num_rows, num_cols, 3), dtype='uint8')\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "#             w, h = 256, 256\n",
    "            w, h = 768, 768\n",
    "#             if i == num_rows - 1: # if at last row\n",
    "#                 h = 128\n",
    "#             if j == num_cols - 1: # if at last column\n",
    "#                 w = 128\n",
    "            \n",
    "            tile_img = in_img.extract_area(128*j,128*i,w,h) # c, r, w, h\n",
    "            tile_img = vips2numpy(tile_img)\n",
    "#             print(tile_img.shape)\n",
    "#             if i == num_rows - 1: # if at last row\n",
    "#                 tile_img = np.pad(tile_img, ((0, 128),(0, 0),(0, 0)), 'constant', constant_values=((0, 0),))\n",
    "       \n",
    "#             if j == num_cols - 1: # if at last column\n",
    "#                 tile_img = np.pad(tile_img, ((0, 0),(0, 128),(0, 0)), 'constant', constant_values=((0, 0),))\n",
    "      \n",
    "            tile_img = Image.fromarray(tile_img, 'RGB')\n",
    "            tile_img = act(tile_img)\n",
    "            tile_img = torch.from_numpy(tile_img)\n",
    "            \n",
    "#             print(tile_img)\n",
    "            tile_img = tile_img.unsqueeze(0)\n",
    "            tile_img = Variable(tile_img)\n",
    "            tile_img = tile_img.cuda()\n",
    "            predict = model(tile_img)\n",
    "\n",
    "            # a = predict.data[0,0].item()\n",
    "            # b = predict.data[0,1].item()\n",
    "            # c = predict.data[0,2].item()\n",
    "#             PRED[i, j, 0] = predict.data[0,0].item()\n",
    "#             PRED[i, j, 1] = predict.data[0,1].item()\n",
    "#             PRED[i, j, 2] = predict.data[0,2].item()\n",
    "            # print('a:',a,'b:',b,'c',c)\n",
    "            # print('predict.data',predict.data)\n",
    "#             _, predict = torch.max(predict.data, 1)\n",
    "            _, p = torch.max(predict.data, 1)\n",
    "            value = p[4].item()\n",
    "#             print(value)\n",
    "            #print(type(value)) #<class 'int'>\n",
    "            nums[i, j] = value\n",
    "      \n",
    "    np.save(SAVE_DATA_DIR + filename.split('.')[-2] + '.npy', nums)\n",
    "#     np.save(SAVE_DATA_DIR + filename.split('.')[-2] + 'value.npy', PRED)\n",
    "    nums = np.repeat(nums[:, :, np.newaxis], 3, axis=2)\n",
    "    #print(nums.shape, nums.dtype)\n",
    "\n",
    "    # nums[:,:,0] = RED, nums[:,:,1] = Green, nums[:,:,2] = Blue\n",
    "    idx_1 = np.where(nums[:,:,0] == 1)  # Index of label 1 (WM)\n",
    "    idx_2 = np.where(nums[:,:,0] == 2)  # Index of label 2 (GM)\n",
    "\n",
    "    # For label 0, leave as black color\n",
    "    # For label 1, set to cyan color: R0G255B255\n",
    "    nums[:,:,0].flat[np.ravel_multi_index(idx_1, nums[:,:,0].shape)] = 0\n",
    "    nums[:,:,1].flat[np.ravel_multi_index(idx_1, nums[:,:,1].shape)] = 255\n",
    "    nums[:,:,2].flat[np.ravel_multi_index(idx_1, nums[:,:,2].shape)] = 255\n",
    "    # For label 2, set to yellow color: R255G255B0\n",
    "    nums[:,:,0].flat[np.ravel_multi_index(idx_2, nums[:,:,0].shape)] = 255\n",
    "    nums[:,:,1].flat[np.ravel_multi_index(idx_2, nums[:,:,1].shape)] = 255\n",
    "    nums[:,:,2].flat[np.ravel_multi_index(idx_2, nums[:,:,2].shape)] = 0\n",
    "\n",
    "    save_img = Image.fromarray(nums, 'RGB')\n",
    "    save_img.save(SAVE_IMG_DIR + filename.split('.')[-2] + '.png')\n",
    "\n",
    "    t.update()\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
